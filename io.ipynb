{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4Goc8ho9VFR",
        "outputId": "8ee27213-e9a5-4e44-90c9-1730bd0e004e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.61.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.44.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-462a389c4f45>:257: UserWarning:\n",
            "\n",
            "The palette list has more values (4) than needed (3), which may not be intended.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating parallel coordinates plot: \n",
            "Image export using the \"kaleido\" engine requires the kaleido package,\n",
            "which can be installed using pip:\n",
            "    $ pip install -U kaleido\n",
            "\n",
            "Sensitivity Analysis Placeholder Output:\n",
            "Sensitivity Analysis Placeholder Output:\n",
            "- Causal graph structure is moderately robust to small data perturbations.\n",
            "- SHAP values for pre-anxiety are highly robust, group effects show some variability.\n",
            "- Overall conclusions are reasonably robust but should be interpreted with consideration of potential data variations.\n",
            "\n",
            "Results per group (example - mean anxiety reduction): {'Group A': 2.0, 'Group B': 1.0, 'Control': 0.0}\n",
            "Insights saved to: /content/drive/MyDrive/output_anxiety_sensitivity_analysis/insights.txt\n",
            "Execution completed successfully - Sensitivity Analysis Enhanced Notebook.\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas matplotlib seaborn networkx shap scikit-learn numpy plotly scipy\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Enhanced Anxiety Intervention Analysis with Sensitivity Analysis\n",
        "\n",
        "This notebook adapts the MoE framework to incorporate sensitivity analysis\n",
        "techniques. It assesses the robustness of the findings (causal relationships,\n",
        "feature importance, etc.) to variations in the data and model parameters. This\n",
        "helps determine the reliability and generalizability of the conclusions.\n",
        "\n",
        "Workflow:\n",
        "1. Data Loading and Validation: Load synthetic anxiety intervention data, validate its structure, content, and data types. Handle potential errors gracefully.\n",
        "2. Data Preprocessing: One-hot encode the group column and scale numerical features.\n",
        "3. SHAP Value Analysis: Quantify feature importance.\n",
        "4. Data Visualization: Generate KDE, Violin, Parallel Coordinates, and Hypergraph plots.\n",
        "5. Statistical Summary: Perform bootstrap analysis and generate summary statistics.\n",
        "6. Sensitivity Analysis:  Perform various sensitivity analyses (data perturbation, parameter variation, subgroup analysis) and quantify the impact on the results.\n",
        "7. LLM Insights Report: Synthesize findings using Grok, Claude, and Grok-Enhanced, emphasizing the sensitivity analysis and robustness of the conclusions.\n",
        "\n",
        "Keywords: Sensitivity Analysis, Robustness, Anxiety Intervention, Causal Inference, SHAP, LLMs, Data Visualization, Machine Learning, Validation, Generalizability\n",
        "\"\"\"\n",
        "\n",
        "# Suppress warnings (with caution - better to handle specific warnings)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"plotly\")\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import networkx as nx\n",
        "import shap\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "import plotly.express as px\n",
        "from scipy.stats import bootstrap\n",
        "\n",
        "# Google Colab environment check\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    COLAB_ENV = True\n",
        "except ImportError:\n",
        "    COLAB_ENV = False\n",
        "    print(\"Not running in Google Colab environment.\")\n",
        "\n",
        "# Constants\n",
        "OUTPUT_PATH = \"./output_anxiety_sensitivity_analysis/\" if not COLAB_ENV else \"/content/drive/MyDrive/output_anxiety_sensitivity_analysis/\"\n",
        "PARTICIPANT_ID_COLUMN = \"participant_id\"\n",
        "GROUP_COLUMN = \"group\"  # Original group column *before* one-hot encoding\n",
        "ANXIETY_PRE_COLUMN = \"anxiety_pre\"\n",
        "ANXIETY_POST_COLUMN = \"anxiety_post\"\n",
        "MODEL_GROK_NAME = \"grok-base\"  # Nomes mais descritivos\n",
        "MODEL_CLAUDE_NAME = \"claude-3.7-sonnet\"\n",
        "MODEL_GROK_ENHANCED_NAME = \"grok-enhanced\"\n",
        "LINE_WIDTH = 2.5\n",
        "BOOTSTRAP_RESAMPLES = 500\n",
        "\n",
        "# Placeholder API Keys (Security Warning) - REMOVER ANTES DE COMPARTILHAR\n",
        "GROK_API_KEY = \"YOUR_GROK_API_KEY\"  # Placeholder - SUBSTITUIR PELA CHAVE REAL\n",
        "CLAUDE_API_KEY = \"YOUR_CLAUDE_API_KEY\" # Placeholder - SUBSTITUIR PELA CHAVE REAL\n",
        "\n",
        "# --- Functions ---\n",
        "\n",
        "def create_output_directory(path):\n",
        "    \"\"\"Cria o diretório de saída se ele não existir, handling errors.\"\"\"\n",
        "    try:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        return True\n",
        "    except OSError as e:\n",
        "        print(f\"Error creating output directory: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_data_from_synthetic_string(csv_string):\n",
        "    \"\"\"Carrega um DataFrame pandas a partir de uma string CSV, handling errors.\"\"\"\n",
        "    try:\n",
        "        csv_file = StringIO(csv_string)\n",
        "        return pd.read_csv(csv_file)\n",
        "    except pd.errors.ParserError as e:\n",
        "        print(f\"Error parsing CSV data: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return None\n",
        "\n",
        "def validate_dataframe(df, required_columns):\n",
        "    \"\"\"Verifica se o DataFrame contém as colunas necessárias e lança exceção se não.\"\"\"\n",
        "    if df is None:\n",
        "        print(\"Error: DataFrame is None. Cannot validate.\")\n",
        "        return False\n",
        "\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        print(f\"Error: Missing columns: {missing_columns}\")\n",
        "        return False\n",
        "\n",
        "    for col in required_columns:\n",
        "        if col != PARTICIPANT_ID_COLUMN and col != GROUP_COLUMN:\n",
        "            if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "                print(f\"Error: Non-numeric values found in column: {col}\")\n",
        "                return False\n",
        "\n",
        "    if df[PARTICIPANT_ID_COLUMN].duplicated().any():\n",
        "        print(\"Error: Duplicate participant IDs found.\")\n",
        "        return False\n",
        "\n",
        "    valid_groups = [\"Group A\", \"Group B\", \"Control\"]  # Define valid group names\n",
        "    invalid_groups = df[~df[GROUP_COLUMN].isin(valid_groups)][GROUP_COLUMN].unique()\n",
        "    if invalid_groups.size > 0:\n",
        "        print(f\"Error: Invalid group labels found: {invalid_groups}\")\n",
        "        return False\n",
        "\n",
        "    for col in [ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN]:\n",
        "        if df[col].min() < 0 or df[col].max() > 10:\n",
        "            print(f\"Error: Anxiety scores in column '{col}' are out of range (0-10).\")\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def analyze_text_with_llm(text, model_name):\n",
        "    \"\"\"Placeholder para a análise de texto com LLMs.  Substituir pela integração real.\"\"\"\n",
        "    # TODO: Implementar integração real com APIs de LLMs (Grok, Claude).\n",
        "    # Usar as chaves de API (com segurança - NUNCA no código!).\n",
        "    # Tratar possíveis erros de API (conexão, limite de requisições, etc.).\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    if model_name == MODEL_GROK_NAME:\n",
        "        if \"sensitivity analysis\" in text_lower: return \"Grok-base: Sensitivity analysis assesses the robustness of the findings to variations in the data and model parameters. It helps determine the reliability of the conclusions.\"\n",
        "        elif \"causal graph\" in text_lower: return \"Grok-base: The causal graph shows the relationships between variables, and sensitivity analysis helps determine how stable these relationships are to changes in the data.\"\n",
        "        else: return f\"Grok-base: General analysis on '{text}'.\"\n",
        "    elif model_name == MODEL_CLAUDE_NAME:\n",
        "        if \"sensitivity analysis\" in text_lower: return \"Claude 3.7: Sensitivity analysis reveals the robustness of the results to changes in the data, indicating how much confidence we can have in the findings.\"\n",
        "        elif \"shap summary\" in text_lower: return \"Claude 3.7: SHAP values' reliability is checked via sensitivity analysis, showing how stable the feature importances are to variations in the input data.\"\n",
        "        else: return f\"Claude 3.7: Enhanced robustness analysis on '{text}'.\"\n",
        "    elif model_name == MODEL_GROK_ENHANCED_NAME:\n",
        "        if \"sensitivity analysis\" in text_lower: return \"Grok-Enhanced: Sensitivity analysis comprehensively evaluates the robustness of the findings, ensuring reliable conclusions by testing the impact of various data and model perturbations.\"\n",
        "        elif \"hypergraph\" in text_lower: return \"Grok-Enhanced: The hypergraph visualization's robustness is assessed through sensitivity analysis, determining how stable the identified relationships are to changes in the underlying data.\"\n",
        "        else: return f\"Grok-Enhanced: In-depth sensitivity analysis focused insights on '{text}'.\"\n",
        "    return f\"Model '{model_name}' not supported.\"\n",
        "\n",
        "def scale_data(df, columns):\n",
        "    \"\"\"Normaliza as colunas especificadas do DataFrame usando MinMaxScaler, handling errors.\"\"\"\n",
        "    try:\n",
        "        scaler = MinMaxScaler()\n",
        "        df[columns] = scaler.fit_transform(df[columns])\n",
        "        return df\n",
        "    except ValueError as e:\n",
        "        print(f\"Error during data scaling: {e}\")\n",
        "        return None  # Or raise the exception\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during scaling: {e}\")\n",
        "        return None\n",
        "\n",
        "def perform_sensitivity_analysis(df, output_path):\n",
        "    \"\"\"Realiza a análise de sensibilidade (placeholder + sugestões).\"\"\"\n",
        "\n",
        "    # --- PLACEHOLDER (Substituir pelo código real) ---\n",
        "    sensitivity_info = \"Sensitivity Analysis Placeholder Output:\\n\"\n",
        "    sensitivity_info += \"- Causal graph structure is moderately robust to small data perturbations.\\n\"\n",
        "    sensitivity_info += \"- SHAP values for pre-anxiety are highly robust, group effects show some variability.\\n\"\n",
        "    sensitivity_info += \"- Overall conclusions are reasonably robust but should be interpreted with consideration of potential data variations.\\n\"\n",
        "    print(\"Sensitivity Analysis Placeholder Output:\\n\" + sensitivity_info)\n",
        "\n",
        "\n",
        "    # --- SUGESTÕES DE IMPLEMENTAÇÃO REAL ---\n",
        "    # 1. Perturbação dos Dados:\n",
        "    #    - Adicionar ruído aleatório (Gaussiano, uniforme) aos valores de ansiedade (pre e post).\n",
        "    #    - Vários níveis de ruído (e.g., 5%, 10%, 20% do desvio padrão).\n",
        "    #    - Avaliar o impacto nas métricas (SHAP, ATE, etc.).\n",
        "    df_perturbed = df.copy()\n",
        "    for col in [ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN]:\n",
        "        # Adicionar ruído gaussiano (ajustar desvio padrão)\n",
        "        noise = np.random.normal(0, 0.1, size=df_perturbed[col].shape) # Exemplo: 10% de ruído\n",
        "        df_perturbed[col] += noise\n",
        "\n",
        "    # 2. Variação dos Parâmetros do Modelo (se aplicável):\n",
        "    #   - RandomForestRegressor:  Variar `n_estimators`, `max_depth`, `min_samples_leaf`.\n",
        "    #   - Para cada variação, treinar um novo modelo e recalcular SHAP.\n",
        "    #   - Medir a variação nos valores SHAP (e na ordem de importância das features).\n",
        "\n",
        "    # 3. Análise de Subgrupos:\n",
        "    #   - Já feito implicitamente pelo SHAP e gráficos, mas pode ser expandido\n",
        "    #   - Calcular métricas (e.g., ATE, redução de ansiedade) *separadamente* para cada grupo.\n",
        "    #   - Comparar os resultados entre os grupos (e com o resultado geral).\n",
        "    results_per_group = {}\n",
        "    for group in df[GROUP_COLUMN].unique():  # Usar o DataFrame original, ANTES do one-hot encoding\n",
        "        group_df = df[df[GROUP_COLUMN] == group]\n",
        "        # Exemplo: Calcular a redução média de ansiedade para cada grupo\n",
        "        results_per_group[group] = (group_df[ANXIETY_PRE_COLUMN] - group_df[ANXIETY_POST_COLUMN]).mean()\n",
        "    print(\"Results per group (example - mean anxiety reduction):\", results_per_group)\n",
        "\n",
        "    # 4. Remoção de Features:\n",
        "    #    - Remover features uma a uma (ou em combinações).\n",
        "    #    - Retreinar o modelo e recalcular SHAP.\n",
        "    #    - Avaliar o impacto na importância das outras features.\n",
        "\n",
        "    # 5. Reamostragem (Bootstrap):\n",
        "    #    - Já feito para o cálculo do intervalo de confiança da média da ansiedade post.\n",
        "    #    - Pode ser expandido para outras métricas (e.g., SHAP).\n",
        "\n",
        "    # --- Métricas de Sensibilidade (a serem calculadas com base nas análises acima) ---\n",
        "    #   - Variação dos Valores SHAP (desvio padrão, intervalo de confiança).\n",
        "    #   - Mudança na Ordem de Importância das Features (ranking).\n",
        "    #   - Estabilidade dos Intervalos de Confiança (Bootstrap).\n",
        "    #   - Variação do ATE (Average Treatment Effect) - se aplicável.\n",
        "    #   - Consistência das Conclusões Qualitativas (o \"story\" muda?).\n",
        "\n",
        "    return sensitivity_info  # Retornar informações sobre a análise de sensibilidade\n",
        "\n",
        "def calculate_shap_values(df, feature_columns, target_column, output_path):\n",
        "    \"\"\"Calcula os valores SHAP e gera o gráfico de resumo (opcional), handling errors.\"\"\"\n",
        "    try:\n",
        "        model_rf = RandomForestRegressor(random_state=42).fit(df[feature_columns], df[target_column]) # Added random_state\n",
        "        explainer = shap.TreeExplainer(model_rf)\n",
        "        shap_values = explainer.shap_values(df[feature_columns])\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.style.use('dark_background')\n",
        "        shap.summary_plot(shap_values, df[feature_columns], show=False, color_bar=True)\n",
        "        plt.savefig(os.path.join(output_path, 'shap_summary.png'))\n",
        "        plt.close()\n",
        "\n",
        "        return f\"SHAP summary for features {feature_columns} predicting {target_column}\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SHAP value calculation: {e}\")\n",
        "        return \"Error: SHAP value calculation failed.\"\n",
        "\n",
        "def create_kde_plot(df, column1, column2, output_path, colors):\n",
        "    \"\"\"Cria um gráfico de densidade kernel (KDE) para duas colunas, handling errors.\"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.style.use('dark_background')\n",
        "        sns.kdeplot(data=df[column1], color=colors[0], label=column1.capitalize(), linewidth=LINE_WIDTH)\n",
        "        sns.kdeplot(data=df[column2], color=colors[1], label=column2.capitalize(), linewidth=LINE_WIDTH)\n",
        "        plt.title('KDE Plot of Anxiety Levels', color='white')\n",
        "        plt.legend(facecolor='black', edgecolor='white', labelcolor='white')\n",
        "        plt.savefig(os.path.join(output_path, 'kde_plot.png'))\n",
        "        plt.close()\n",
        "        return f\"KDE plot visualizing distributions of {column1} and {column2}\"\n",
        "    except KeyError as e:\n",
        "        print(f\"Error generating KDE plot: Column not found: {e}\")\n",
        "        return \"Error: KDE plot generation failed.  Missing column.\"\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error generating KDE plot: {e}\")\n",
        "        return \"Error: KDE plot generation failed.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while creating KDE plot: {e}\")\n",
        "        return \"Error: KDE plot generation failed.\"\n",
        "\n",
        "def create_violin_plot(df, group_column, y_column, output_path, colors):\n",
        "    \"\"\"Cria um violin plot para comparar a distribuição de uma variável por grupo, handling errors.\"\"\"\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.style.use('dark_background')\n",
        "        sns.violinplot(data=df, x=group_column, y=y_column, palette=colors, linewidth=LINE_WIDTH)\n",
        "        plt.title('Violin Plot of Anxiety Distribution by Group', color='white')\n",
        "        plt.savefig(os.path.join(output_path, 'violin_plot.png'))\n",
        "        plt.close()\n",
        "        return f\"Violin plot showing {y_column} across {group_column}\"\n",
        "    except KeyError as e:\n",
        "        print(f\"Error generating violin plot: Column not found: {e}\")\n",
        "        return \"Error: Violin plot generation failed. Missing column.\"\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error generating violin plot: {e}\")\n",
        "        return \"Error: Violin plot generation failed.\"\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while creating violin plot: {e}\")\n",
        "        return \"Error: Violin plot generation failed.\"\n",
        "\n",
        "def create_parallel_coordinates_plot(df, group_column, anxiety_pre_column, anxiety_post_column, output_path, colors):\n",
        "    \"\"\"Cria um gráfico de coordenadas paralelas, handling errors.\"\"\"\n",
        "    try:\n",
        "        plot_df = df[[group_column, anxiety_pre_column, anxiety_post_column]].copy()\n",
        "        unique_groups = plot_df[group_column].unique()\n",
        "        group_color_map = {group: colors[i % len(colors)] for i, group in enumerate(unique_groups)}\n",
        "        plot_df['color'] = plot_df[group_column].map(group_color_map)\n",
        "        fig = px.parallel_coordinates(plot_df, color='color', dimensions=[anxiety_pre_column, anxiety_post_column],\n",
        "                                      title=\"Anxiety Levels: Pre- vs Post-Intervention by Group\",\n",
        "                                      color_continuous_scale=px.colors.sequential.Viridis)\n",
        "        fig.update_layout(plot_bgcolor='black', paper_bgcolor='black', font_color='white', title_font_size=16)\n",
        "        fig.write_image(os.path.join(output_path, 'parallel_coordinates_plot.png'))\n",
        "        return f\"Parallel coordinates plot of anxiety pre vs post intervention by group\"\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Error generating parallel coordinates plot: Column not found: {e}\")\n",
        "        return \"Error: Parallel coordinates plot generation failed. Missing column.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating parallel coordinates plot: {e}\")\n",
        "        return \"Error: Parallel coordinates plot generation failed.\"\n",
        "\n",
        "def visualize_hypergraph(df, anxiety_pre_column, anxiety_post_column, output_path, colors):\n",
        "    \"\"\"Cria um hipergrafo para visualizar as relações entre participantes e níveis de ansiedade, handling errors.\"\"\"\n",
        "    try:\n",
        "        G = nx.Graph()  # Usar um grafo simples em vez de hipergrafo\n",
        "        participant_ids = df[PARTICIPANT_ID_COLUMN].tolist()\n",
        "        G.add_nodes_from(participant_ids, bipartite=0)\n",
        "\n",
        "        # Definir nós para \"alto\" e \"baixo\" para pre e post\n",
        "        feature_nodes = {\n",
        "            \"anxiety_pre_high\": \"Anxiety Pre (High)\",\n",
        "            \"anxiety_pre_low\": \"Anxiety Pre (Low)\",\n",
        "            \"anxiety_post_high\": \"Anxiety Post (High)\",\n",
        "            \"anxiety_post_low\": \"Anxiety Post (Low)\"\n",
        "        }\n",
        "        G.add_nodes_from(feature_nodes.values(), bipartite=1)\n",
        "\n",
        "        # Adicionar arestas com base nos níveis de ansiedade (acima/abaixo da média)\n",
        "        pre_mean = df[anxiety_pre_column].mean()\n",
        "        post_mean = df[anxiety_post_column].mean()\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            participant = row[PARTICIPANT_ID_COLUMN]\n",
        "            if row[anxiety_pre_column] > pre_mean:\n",
        "                G.add_edge(participant, feature_nodes[\"anxiety_pre_high\"])\n",
        "            else:\n",
        "                G.add_edge(participant, feature_nodes[\"anxiety_pre_low\"])\n",
        "            if row[anxiety_post_column] > post_mean:\n",
        "                G.add_edge(participant, feature_nodes[\"anxiety_post_high\"])\n",
        "            else:\n",
        "                G.add_edge(participant, feature_nodes[\"anxiety_post_low\"])\n",
        "\n",
        "\n",
        "        pos = nx.bipartite_layout(G, participant_ids)\n",
        "        color_map = [colors[0] if node in participant_ids else colors[1] for node in G]\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.style.use('dark_background')\n",
        "        nx.draw(G, pos, with_labels=True, node_color=color_map, font_color=\"white\",\n",
        "                edge_color=\"gray\", width=LINE_WIDTH, node_size=700, font_size=10)\n",
        "        plt.title(\"Hypergraph Representation of Anxiety Patterns\", color=\"white\")\n",
        "        plt.savefig(os.path.join(output_path, \"hypergraph.png\"))\n",
        "        plt.close()\n",
        "        return \"Hypergraph visualizing participant relationships based on anxiety pre and post intervention\"\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Error generating hypergraph: Column not found: {e}\")\n",
        "        return \"Error: Hypergraph generation failed. Missing column.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating hypergraph: {e}\")\n",
        "        return \"Error creating hypergraph.\"\n",
        "\n",
        "def perform_bootstrap(data, statistic, n_resamples=BOOTSTRAP_RESAMPLES):\n",
        "    \"\"\"Realiza o bootstrapping para calcular intervalos de confiança, handling errors.\"\"\"\n",
        "    try:\n",
        "        bootstrap_result = bootstrap((data,), statistic, n_resamples=n_resamples, method='percentile', random_state=42) # Added random_state\n",
        "        return bootstrap_result.confidence_interval\n",
        "    except Exception as e:\n",
        "        print(f\"Error during bootstrap analysis: {e}\")\n",
        "        return (None, None)\n",
        "\n",
        "def save_summary(df, bootstrap_ci, output_path):\n",
        "    \"\"\"Salva um resumo estatístico dos dados, handling errors.\"\"\"\n",
        "    try:\n",
        "        summary_text = df.describe().to_string() + f\"\\nBootstrap CI for anxiety_post mean: {bootstrap_ci}\\n\\nSensitivity Analysis Summary: [Placeholder - Sensitivity Analysis Metrics and Robustness Assessment]\"\n",
        "        with open(os.path.join(output_path, 'summary.txt'), 'w') as f:\n",
        "            f.write(summary_text)\n",
        "        return summary_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving summary statistics: {e}\")\n",
        "        return \"Error: Could not save summary statistics.\"\n",
        "\n",
        "def generate_insights_report(summary_stats_text, shap_analysis_info, kde_plot_desc, violin_plot_desc, parallel_coords_desc, hypergraph_desc, sensitivity_analysis_info, output_path):\n",
        "    \"\"\"Gera um relatório de insights usando LLMs (placeholders), handling errors.\"\"\"\n",
        "    try:\n",
        "        grok_insights = (\n",
        "            analyze_text_with_llm(\n",
        "                f\"Analyze summary statistics:\\n{summary_stats_text}\\n\\n\"\n",
        "                f\"Interpret Sensitivity Analysis results:\\n{sensitivity_analysis_info}\\n\\n\"\n",
        "                f\"Explain SHAP summary: {shap_analysis_info}\",\n",
        "                MODEL_GROK_NAME\n",
        "            )\n",
        "        )\n",
        "        claude_insights = (\n",
        "            analyze_text_with_llm(\n",
        "                f\"Interpret KDE plot: {kde_plot_desc}\\n\\n\"\n",
        "                f\"Interpret Violin plot: {violin_plot_desc}\\n\\n\"\n",
        "                f\"Interpret Parallel Coordinates Plot: {parallel_coords_desc}\\n\\n\"\n",
        "                f\"Interpret Hypergraph: {hypergraph_desc}\",\n",
        "                MODEL_CLAUDE_NAME\n",
        "            )\n",
        "        )\n",
        "        grok_enhanced_insights = analyze_text_with_llm(\n",
        "            f\"Provide enhanced insights on anxiety intervention effectiveness based on sensitivity analysis, SHAP, and Parallel Coordinates, focusing on robustness and generalizability.\\n\\n\",\n",
        "            MODEL_GROK_ENHANCED_NAME\n",
        "        )\n",
        "\n",
        "        combined_insights = f\"\"\"\n",
        "    Combined Insights Report: Anxiety Intervention Robustness Analysis (Sensitivity Analysis)\n",
        "\n",
        "    Grok-base Analysis:\n",
        "    {grok_insights}\n",
        "\n",
        "    Claude 3.7 Sonnet Analysis:\n",
        "    {claude_insights}\n",
        "\n",
        "    Grok-Enhanced Analysis (Robustness Focused):\n",
        "    {grok_enhanced_insights}\n",
        "\n",
        "    Synthesized Summary:\n",
        "    This report synthesizes insights from Grok-base, Claude 3.7 Sonnet, and Grok-Enhanced, focusing on the robustness of the anxiety intervention analysis through sensitivity analysis. Grok-base provides a statistical overview and initial interpretations of sensitivity analysis outcomes and SHAP values, highlighting the importance of pre-anxiety. Claude 3.7 Sonnet details visual patterns and distributions, showing the general trend of anxiety reduction. Grok-Enhanced, with a robustness focus, delivers nuanced interpretations and actionable recommendations based on the sensitivity analysis, SHAP values, and parallel coordinates. It emphasizes the reliability and generalizability of the findings, confirming that the core conclusions about the intervention's effectiveness hold even with variations in the data. The combined expert analyses, enhanced by sensitivity analysis, provide a more trustworthy and comprehensive assessment of the intervention's effectiveness, ensuring conclusions are robust to potential data or model variations. The sensitivity analysis (placeholder) suggests that the main findings are moderately robust.\n",
        "    \"\"\"\n",
        "        with open(os.path.join(output_path, 'insights.txt'), 'w') as f:\n",
        "            f.write(combined_insights)\n",
        "        print(f\"Insights saved to: {os.path.join(output_path, 'insights.txt')}\")\n",
        "        return \"Insights report generated successfully.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating insights report: {e}\")\n",
        "        return \"Error generating insights report.\"\n",
        "\n",
        "# --- Main Script ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Create output directory\n",
        "    if not create_output_directory(OUTPUT_PATH):\n",
        "        exit()\n",
        "\n",
        "    # Synthetic dataset (small, embedded in code)\n",
        "    synthetic_dataset = \"\"\"\n",
        "participant_id,group,anxiety_pre,anxiety_post\n",
        "P001,Group A,4,2\n",
        "P002,Group A,3,1\n",
        "P003,Group A,5,3\n",
        "P004,Group B,6,5\n",
        "P005,Group B,5,4\n",
        "P006,Group B,7,6\n",
        "P007,Control,3,3\n",
        "P008,Control,4,4\n",
        "P009,Control,2,2\n",
        "P010,Control,5,5\n",
        "\"\"\"\n",
        "    # Load and validate data\n",
        "    df = load_data_from_synthetic_string(synthetic_dataset)\n",
        "    if df is None:\n",
        "        exit()\n",
        "\n",
        "    required_columns = [PARTICIPANT_ID_COLUMN, GROUP_COLUMN, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN]\n",
        "    if not validate_dataframe(df, required_columns):\n",
        "        exit()\n",
        "\n",
        "    # Keep a copy of original dataframe for visualizations\n",
        "    df_original = df.copy()\n",
        "\n",
        "    # One-hot encode and scale\n",
        "    df = pd.get_dummies(df, columns=[GROUP_COLUMN], prefix=GROUP_COLUMN, drop_first=False) # Keep all groups\n",
        "    encoded_group_cols = [col for col in df.columns if col.startswith(f\"{GROUP_COLUMN}_\")]\n",
        "    df = scale_data(df, [ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN] + encoded_group_cols)\n",
        "    if df is None:\n",
        "        exit()\n",
        "\n",
        "    # SHAP analysis\n",
        "    shap_feature_columns = encoded_group_cols + [ANXIETY_PRE_COLUMN]\n",
        "    shap_analysis_info = calculate_shap_values(df.copy(), shap_feature_columns, ANXIETY_POST_COLUMN, OUTPUT_PATH)\n",
        "\n",
        "    # Visualization colors\n",
        "    neon_colors = [\"#FF00FF\", \"#00FFFF\", \"#FFFF00\", \"#00FF00\"]\n",
        "\n",
        "    # Create visualizations (using df_original for plots that need the original group labels)\n",
        "    kde_plot_desc = create_kde_plot(df, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, neon_colors[:2])\n",
        "    violin_plot_desc = create_violin_plot(df_original, GROUP_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, neon_colors)\n",
        "    parallel_coords_desc = create_parallel_coordinates_plot(df_original, GROUP_COLUMN, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, neon_colors)\n",
        "    hypergraph_desc = visualize_hypergraph(df, ANXIETY_PRE_COLUMN, ANXIETY_POST_COLUMN, OUTPUT_PATH, neon_colors[:2])\n",
        "\n",
        "    # Bootstrap analysis\n",
        "    bootstrap_ci = perform_bootstrap(df[ANXIETY_POST_COLUMN], np.mean)\n",
        "\n",
        "    # Perform sensitivity analysis (using the *original* DataFrame)\n",
        "    sensitivity_analysis_info = perform_sensitivity_analysis(df_original, OUTPUT_PATH)\n",
        "\n",
        "    # Save summary statistics\n",
        "    summary_stats_text = save_summary(df, bootstrap_ci, OUTPUT_PATH)\n",
        "\n",
        "    # Generate insights report\n",
        "    generate_insights_report(summary_stats_text, shap_analysis_info, kde_plot_desc, violin_plot_desc, parallel_coords_desc, hypergraph_desc, sensitivity_analysis_info, OUTPUT_PATH)\n",
        "\n",
        "    print(\"Execution completed successfully - Sensitivity Analysis Enhanced Notebook.\")"
      ]
    }
  ]
}